[
  {
    "objectID": "posts/2026-01-27-hello-world/index.html",
    "href": "posts/2026-01-27-hello-world/index.html",
    "title": "The First Post",
    "section": "",
    "text": "credit: muvamo.com\n\n\nOver a year ago, my wife and I moved from Singapore to Vienna. While settling down in this new city, I found time to read more, think more, and code more. With the freedom to explore, I found myself easily excited with every new idea I encountered. Looking back on 2025, I realised that I started many projects, but rarely finished any (satisfactorily). I needed a better system. Hence, this hastily constructed static page I proudly call my blog.\nThis blog is a place for me to put my thoughts down, and hopefully a place for you to pick something up. Here‚Äôs to completing more projects in the coming yea-"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A Work In Progress",
    "section": "",
    "text": "Welcome to my blog where I explore technical topics that interest me. I write about projects, experiments, and ideas I‚Äôm working on."
  },
  {
    "objectID": "index.html#hi-im-gabriel",
    "href": "index.html#hi-im-gabriel",
    "title": "A Work In Progress",
    "section": "",
    "text": "Welcome to my blog where I explore technical topics that interest me. I write about projects, experiments, and ideas I‚Äôm working on."
  },
  {
    "objectID": "index.html#recent-posts",
    "href": "index.html#recent-posts",
    "title": "A Work In Progress",
    "section": "Recent Posts",
    "text": "Recent Posts\n\n\n\n\n\n\n\n\n\n\nOrbital Data Centres: Mission Analysis\n\n\n\nAdroitly\n\nMission Analysis\n\n\n\nHas the AI hype train achieved escape velocity? An analysis of SpaceX‚Äôs orbital data centre for AGI training\n\n\n\n\n\nFeb 15, 2026\n\n\nGabriel\n\n\n\n\n\n\n\n\n\n\n\n\nSatellite Tasking MCP Server\n\n\n\nAdroitly\n\nMCP\n\noptimisation\n\n\n\nBuilding a Model Context Protocol server for satellite image tasking with natural language and agentic workflows\n\n\n\n\n\nJan 30, 2026\n\n\nGabriel\n\n\n\n\n\n\n\n\n\n\n\n\nCan You Beat TOTO? A Statistical Analysis of Singapore‚Äôs National Lottery\n\n\n\nData Analysis\n\nStatistics\n\nProbability\n\nSingapore\n\n\n\nAnalyzing 17 years of TOTO draw data to determine if lottery numbers can be predicted and whether the house always wins\n\n\n\n\n\nJan 27, 2026\n\n\nGabriel\n\n\n\n\n\n\n\n\n\n\n\n\nThe First Post\n\n\n\n\n\nNew Year, New Me\n\n\n\n\n\nJan 27, 2026\n\n\nGabriel\n\n\n\n\n\nNo matching items\n\nView more blog posts ‚Üí | Company Showcase ‚Üí"
  },
  {
    "objectID": "adroitly/2026-02-15-orbital-data-centres/index.html",
    "href": "adroitly/2026-02-15-orbital-data-centres/index.html",
    "title": "Orbital Data Centres: Mission Analysis",
    "section": "",
    "text": "credit: Grok"
  },
  {
    "objectID": "adroitly/2026-02-15-orbital-data-centres/index.html#mission-phases",
    "href": "adroitly/2026-02-15-orbital-data-centres/index.html#mission-phases",
    "title": "Orbital Data Centres: Mission Analysis",
    "section": "Mission Phases",
    "text": "Mission Phases\nWhat does it take to get to AGI in space? We see 4 key phases of the mission:\n\n\nDesign, Launch, and Deployment. The data centers need to be designed, launched into space, and establish connectivity with each other and the xAI engineers on the ground.\nBring Your Own Data. The training data needs to be accessible by the data centers to begin training. This deserves its own phase of the mission, with interesting challenges and multiple candidate solutions.\nTraining. This phase encompasses all training stages: pre-training, fine-tuning, and reinforcement learning with human feedback.\nModel Serving. Assuming AGI has been created, remains subservient (and did not just take control and fly off to the stars), there are now networking challenges to dispense its infinite wisdom to the world."
  },
  {
    "objectID": "adroitly/2026-02-15-orbital-data-centres/index.html#what-orbital-regime",
    "href": "adroitly/2026-02-15-orbital-data-centres/index.html#what-orbital-regime",
    "title": "Orbital Data Centres: Mission Analysis",
    "section": "What orbital regime?",
    "text": "What orbital regime?\nWith SpaceX‚Äôs fleet of rockets, access to a launch vehicle and most orbital regimes is a non-issue. To leverage the network connectivity of the Starlink constellation, placing the data centre compute at a similar altitude is preferable as well, in LEO (~500km)."
  },
  {
    "objectID": "adroitly/2026-02-15-orbital-data-centres/index.html#how-many-gpus",
    "href": "adroitly/2026-02-15-orbital-data-centres/index.html#how-many-gpus",
    "title": "Orbital Data Centres: Mission Analysis",
    "section": "How many GPUs?",
    "text": "How many GPUs?\nThere aren‚Äôt many definitive sources on this, so we will proceed with a comparative analysis. Grok-4 was trained in the Colossus data centre with 200,000 H100 GPUs. Let‚Äôs assume that the huge step from current models into true AGI can be achieved with the same number of GPUs, and upgrade the H100 to B200. This represents a 2x to 3x increase in training performance."
  },
  {
    "objectID": "adroitly/2026-02-15-orbital-data-centres/index.html#one-mega-satellite-or-many-satellites",
    "href": "adroitly/2026-02-15-orbital-data-centres/index.html#one-mega-satellite-or-many-satellites",
    "title": "Orbital Data Centres: Mission Analysis",
    "section": "One Mega-Satellite, or Many Satellites?",
    "text": "One Mega-Satellite, or Many Satellites?\nGPUs need to be housed snugly into a satellite bus, which provides electrical power, thermal regulation, and networking. At the limit, one GPU could be packed into one small-satellite. This might not be desirable as there are overheads incurred per satellite - one more set of attitude control subsystem, one more set of communication links, etc.. On the other extreme, all 200,000 GPUs could be placed into a single monolithic structure (i.e.¬†‚Äúa Colossus in orbit‚Äù).\nIn practice, launching a monolithic structure is impractical - Starship is the largest SpaceX rocket and has useable dimensions of 400m^3, while the dimensions of the 200,000 GPUs alone is approximately 38,400m^3 (without supporting satellite bus subsystems).\nSo realistically, our orbital data centre will be launched in the form of many small satellites with intensive data networking."
  },
  {
    "objectID": "adroitly/2026-02-15-orbital-data-centres/index.html#satellite-constellation-architecture",
    "href": "adroitly/2026-02-15-orbital-data-centres/index.html#satellite-constellation-architecture",
    "title": "Orbital Data Centres: Mission Analysis",
    "section": "Satellite Constellation Architecture",
    "text": "Satellite Constellation Architecture\nHow should we configure this satellite formation? The main technical challenge is finding an architecture that enables high speed communication between the GPUs across satellites.\nLets draw inspiration from Google‚Äôs Project Suncatcher, and Starcloud.\n\nProject Suncatcher - Formation Flying Fantasies\nProject Suncatcher is Google research project evaluating its version of orbital data centres. The report proposes a constellation of 81 satellites packed into a rotating circular plane 1km in diameter. The short distances between satellites allow for high inter-satellite networking speeds, and the distributed constellation architecture allows for easy scaling and replacement.\nThe satellite formation is maintained with minimal need for propulsion, exploiting the J2-term in Earth gravitational field (explainer: the Earth is ‚Äúfatter‚Äù about the equator. You are further away from Earth‚Äôs centre, and centrifugal force is reduced, making you weigh less and allowing you to eat more guilt-free. Incidentally, this ‚Äúdisturbance in the force‚Äù perturbs satellites orbits in a predictable manner, which Google exploits in Suncatcher‚Äôs orbit design.)\n\n\n\ncredit: Project Suncatcher\n\n\nProject Suncatcher also studied intersatellite networking technology - while Starlink‚Äôs Optical Inter Satellite Links are capable of 100Gbps speeds, terrestrial data centre Infiniband NDR technology is at 400Gbps, with upcoming XDR technology doubling to 800Gbps (i.e.¬†8x Starlink speeds). In order to get to juicy 800Gbps speeds, Google leveraged the higher Signal-to-Noise Ratios enjoyed at short inter-satellite distances, employing Commercial Off-The-Shelf (COTS) Dense Wavelength Division Multiplexing (DWDM) transceiver technology, similar to that used in terrestrial data centres.\n\n\nStarcloud - Big Backbones\nStarcloud is the current market leader in the orbital data centre race, having launched its prototype mission in November 2025, comprising a single H100 GPU (we have a long way to go).\nStarcloud proposes a different architecture - after satellites are launched into orbit, they physically plug into a physical structure that‚Äôs already in orbit. This physical structure provides the networking backbone for high data centre speeds, eliminating the need for tight orbit design and laser links.\n\n\n\ncredit: Starcloud\n\n\n\n\n\ncredit: Starcloud\n\n\nHow will the massive solar array and networking backbone get into orbit? ü§∑\nThis approach avoids the challenges of intersatellite links and tight orbital formation, at the cost of conducting in-space assembly of a 4km megastructure. For context, the International Space Station is the largest manmade structure currently, at 109m. Yes, Starcloud could point to proven automated Rendezvous, Proximity Operations, and Docking (RPOD) techniques as justification. But what about the massive solar array? With today‚Äôs approaches, this implies some form of in-space assembly technology, or precision thrusters on solar panel segments to RPOD with each other. Another challenge: how to adjust orientation to keep the solar array oriented towards the Sun?\nTaken as a whole, the ‚Äúassemble small satellites into a monolith‚Äù approach faces more (and tougher) technical challenges. And if they succeed, you can be sure the astronomers will come complaining."
  },
  {
    "objectID": "adroitly/2026-02-15-orbital-data-centres/index.html#putting-gpus-into-a-box---satellite-packaging",
    "href": "adroitly/2026-02-15-orbital-data-centres/index.html#putting-gpus-into-a-box---satellite-packaging",
    "title": "Orbital Data Centres: Mission Analysis",
    "section": "Putting GPUs into a Box - Satellite Packaging",
    "text": "Putting GPUs into a Box - Satellite Packaging\nWe‚Äôve talked about how satellites could potentially be configured around each other. But what about the design of each satellite? More importantly, how many GPUs can fit on a small satellite? There are several design aspects to consider:\n\nThermal\nAn Nvidia B200 draws 1200W of electrical power and will eventually be converted to heat. In the vacuum of space, there is no atmosphere to facilitate heat dispersal through conduction or convection (and also why no one can hear you scream). Radiation is the remaining means of heat dissipation, which is governed by Stefan-Boltzmann‚Äôs Law. For reference, Starlink Block v3 measures 7m x 3.5m = 24.5m^2 top panel area. But we should also note that there are many other heat generating modules in Starlink that need to radiate out through the same area, so adding 10 GPUs on a Starlink satellite requires approximately double the current surface area.\n\n\n\n\n\n\n\n\n\n\n\nPower and Mechanical\nThe more GPUs are hosted per satellite, the more solar panels are required to generate the required electrical power. A related design parameter is the orbit - Dawn-Dusk Sun Synchronous Orbits are capable of getting constant Sun exposure at LEO altitudes, allowing for constant electrical power generation. While this suggests that batteries might not be needed, in reality, load balancing concerns might mean a small battery is still required.\n\n\n\n\n\n\n\n\n\nFor reference, Starcloud‚Äôs proposed 4km megastructure would generate approximately 6.5GW of power with the same calculations. Starcloud claims a 5GW data centre, which suggests that 1.5GW is budgeted for efficiency losses and non-workload, typical spacecraft tasks.\nLarge-area solar panels can cause a satellite to be ungainly and challenging to steer - in physics terminology, the Moment of Inertia grows. The intuition can be illustrated by imagining an ice skater spinning on the ice - to slow her rotation, she extends her arms. Large solar panels are akin to extended arms, making it difficult for the satellite to orient itself to track the Sun.\nAdditionally, large solar panels need to be folded into a small, compact volume to fit into the rocket‚Äôs payload fairing. The larger the solar panels, the more folds are required, presenting more mechanical challenges in securing the panels during launch, and stiffening the deployed panels in-orbit.\n\n\nSo‚Ä¶ How Many Satellites Again?\nThere are many feasible solutions, which have to account for the Thermal, Power, and Mechanical considerations stated above. Handwaving-ly, we can assume that for cost effectiveness the satellite would be built on a modified Starlink assembly line and take its approximate dimensions and solar power generation (currently ~3000W). That amounts to 2 B200 GPUs, which conveniently fits the GB200 Superchip (up to 2700W).\nThis suggests the orbital data centre will be made up of 200,000 / 2 = 100,000 satellites. Wow.\nReferencing Google‚Äôs Project Suncatcher again - recall that meter-level distances were required to achieve the intersatellite link speeds required for data centre link speeds. With 100,000 satellites, networking might resurface as a technical challenge. If anyone can do it, SpaceX can.\n\n\nHonourable Mention - Radiation\nWhile this section is primarily focused on estimating the number of satellites by counting how many GPUs can fit on a single satellite, there is another practical consideration that we will briefly mention - the chip fabrication process affects its survivability in Low Earth Orbit‚Äôs radiation environment. Today‚Äôs typical space-grade chips use transistors made on the 20nm process, while Nvidia‚Äôs B200 are made on TSMC‚Äôs 4nm process. The smaller the process node, the more susceptible the chip‚Äôs operations become to radiation effects. Project Suncatcher performed a more in-depth study on this aspect if you‚Äôre interested."
  },
  {
    "objectID": "adroitly/2026-02-15-orbital-data-centres/index.html#how-much-data",
    "href": "adroitly/2026-02-15-orbital-data-centres/index.html#how-much-data",
    "title": "Orbital Data Centres: Mission Analysis",
    "section": "How Much Data?",
    "text": "How Much Data?\nWe will limit our discussion to data that is need to train AGI. As the scaling law suggests, that‚Äôs basically all the data. Experts might quibble about synthetic data generation, data selectivity and quality, but let‚Äôs assume bringing ‚ÄúThe Global Datasphere‚Äù up into orbit as a starting point. This is defined as all digital data (text, video, etc.) created, which is currently estimated at 175 zettabytes = 175,000,000,000 TB.\nAmazon offers data centre migration services in the form of a tricked out compute + harddrive called the Amazon Snowball at 210 TB. That makes the datasphere equivalent to 833,333,333 Snowballs. Phew.\nIn practice, some selectivity is probably needed, reducing the data size. It‚Äôs challenging to define a realistic number for this, so let‚Äôs proceed and treat this as a worst case scenario.\n\n\n\nA doggy playing in the snow. credit: AWS\n\n\n\nThe Initial Setup: Getting the Datasphere into Orbit\nTaking cue from Amazon, large dataset migration on-ground is accomplished physically. Given the large ground-to-orbit distances attenuating communication bandwidth (more on this later), it is likely data will be physically launched into orbit as well.\nDistributing our 833,333,333 Snowballs across 100,000 satellites means approximately 8333 Snowballs per satellite. The data would weigh 187 tons per satellite, which is clearly unreasonable.\nThis suggests that on top of ‚Äúcompute‚Äù satellites, we will now have to contend with dedicated ‚Äúdata‚Äù satellites that supplement the data storage needs of AGI training."
  },
  {
    "objectID": "adroitly/2026-02-15-orbital-data-centres/index.html#we-still-need-ground-space-communication",
    "href": "adroitly/2026-02-15-orbital-data-centres/index.html#we-still-need-ground-space-communication",
    "title": "Orbital Data Centres: Mission Analysis",
    "section": "We still need Ground-Space Communication",
    "text": "We still need Ground-Space Communication\nEven if SpaceX has the capability to launch such quantities of satellites at a high cadence, the datasphere is only going to grow larger. Furthermore, there are will be continually updated datasets to train a new version of the AGI, or for post-training. While not on the zettabyte scale, we‚Äôre still in terrabyte or petabyte territory. That‚Äôs where Ground-Space communication links come into play.\nThere are two main communication modes to consider: Radio Frequency (RF) and Optical (i.e.¬†Laser).\nThere are two challenges to overcome: distance, and the atmosphere.\n\nRF versus Laser Comms\nData rate. At a fundamental level, data rate is constrained by Shannon-Hartley‚Äôs Theorem,\n\\[\nC \\propto B\n\\]\nThe channel capacity (bits per second) is proportional to the bandwidth of the channel (Hz), for a given Signal to Noise Ratio. Laser links have bandwidths on the order of THz, while RF bandwidths are on the order of GHz. Laser wins, theoretically. Starlink uses RF communication in the Ku-band up to 100Mbps, while NASA‚Äôs TBIRD has demonstrated laser communication speeds of up to 200Gbps.\nAtmospheric Effects. Rain and clouds affect both RF and laser, but severely attenuates laser more. Typical ground to space laser communication demonstrations have required clear skies. RF communications can operate in gloomier weather, making it more operationally robust.\n\n\nProject Loon: A Robust Hybrid Solution\nGoogle‚Äôs Project Loon utilised a hybrid solution. The key insight was that much of atmospheric attenuation was experienced from sea-level up to 20km altitude (i.e.¬†the stratosphere). A high-altitude balloon was stationed at 18km to 25km altitude. For robust communications, RF links were used down to the ground, while laser links were used for communication beyond the stratosphere.\nThe project was initially planned to bring connectivity to rural areas, but was sadly shut down due to the lack of interest.\nIn the AGI age, there is new need for very high speed ground to space links. As with hot air, hope may rise again."
  },
  {
    "objectID": "adroitly/2026-02-15-orbital-data-centres/index.html#agi-applications",
    "href": "adroitly/2026-02-15-orbital-data-centres/index.html#agi-applications",
    "title": "Orbital Data Centres: Mission Analysis",
    "section": "AGI Applications",
    "text": "AGI Applications\nTo motivate the need for high bandwidth communications, let‚Äôs briefly discuss the promise of AGI. In the near term, AGI is expected to perform where current frontier models stutter - long horizon tasks, cross-domain knowledge synthesis and transfer for novel discoveries, and general decision making under uncertainty. These capabilities are easily applicable to many areas in science, robotics, and even business. Applications in these areas require high bandwidth communications to control equipment for drug discovery, to take video and other sensory inputs from robots and control them in real time, or to ingest large corpus of real time stock market and business news in order to make decisions. Compared to primarily text based inputs in today‚Äôs LLMs, a single instance of the AGI model would already require Gbps speeds (particularly for robot teleoperation); let alone multiple instances to serve customers across the world."
  },
  {
    "objectID": "adroitly/2026-02-15-orbital-data-centres/index.html#starlink-could-be-the-bottleneck",
    "href": "adroitly/2026-02-15-orbital-data-centres/index.html#starlink-could-be-the-bottleneck",
    "title": "Orbital Data Centres: Mission Analysis",
    "section": "Starlink Could Be the Bottleneck",
    "text": "Starlink Could Be the Bottleneck\nThus far, there has been relaxed latency requirements for the ad-hoc data transfer phase and the training phase. However model serving for multimodal and robotics control applications would now demand low latency. Taking reference to terrestrial data centres, we are looking at Tbps speeds.\nAt 100Gbps per laser link, we are talking about 10+ dedicated links at any time to achieve Tbps on aggregate. For a data centre in LEO, this might be challenging - but reconfiguring Starlink orbits could be a simple solution.\nAdditionally, given our initial estimates of a 100,000 satellite data centre, the sheer physical area taken up by the data centre might be large enough to easily establish contact with enough Starlink satellites for the required Tbps speeds."
  },
  {
    "objectID": "adroitly/2026-02-15-orbital-data-centres/index.html#what-about-the-moon",
    "href": "adroitly/2026-02-15-orbital-data-centres/index.html#what-about-the-moon",
    "title": "Orbital Data Centres: Mission Analysis",
    "section": "What About the Moon?",
    "text": "What About the Moon?\nOn 9 Feb 2026, Elon announced a shift in SpaceX‚Äôs focus from Mars to the Moon. This could be another argument for AGI in space - placing an AGI data centre at the Earth-Moon L1 Lagrange point could act as an autonomous Mission Control for rockets launched towards the Moon. The AGI at L1 could also supervise exploration and construction activities on the Moon.\n\n\n\ncredit: Gustavo Gargioni"
  },
  {
    "objectID": "adroitly/2026-02-15-orbital-data-centres/index.html#can-we-utilise-the-orbital-data-centre",
    "href": "adroitly/2026-02-15-orbital-data-centres/index.html#can-we-utilise-the-orbital-data-centre",
    "title": "Orbital Data Centres: Mission Analysis",
    "section": "Can We Utilise the Orbital Data Centre?",
    "text": "Can We Utilise the Orbital Data Centre?\nAssuming all the technical challenges have been overcome, the allure of an orbital data centre is the easy access to ‚Äúfree‚Äù power, from the Sun. Could the compute in space be used by us mere mortals to train our models more cheaply?\nFor terrestrial applications, the projected increase in power supply suggests that the boring approach of training your model in a terrestrial data centre might be getting cheaper in the near future. If you truly want to obtain that ‚ÄúMade in Space‚Äù bragging right, you‚Äôd have to send your data up into orbit through Loon and Starlink, and you can be sure that you‚Äôll be sharing at least some of those infrastructure costs.\nBut if you are in the remote sensing / Earth observation business, data centres in orbit changes the game. Modern remote sensing satellites today spend R&D resources to build custom edge AI solutions to process data-heavy imagery/raw data onboard the satellite, in an effort to avoid the slow downlink speeds caused by atmospheric attenuation. With an orbiting data centres, a commercial laser link terminal is all you need to send your data for processing. By avoiding the limited hardware resources of an edge processor, more complex models can be used, improving performance."
  },
  {
    "objectID": "adroitly.html#showcase",
    "href": "adroitly.html#showcase",
    "title": "Adroitly Consulting",
    "section": "Showcase",
    "text": "Showcase\n\n\n\n\n\n\n\n\n\n\nOrbital Data Centres: Mission Analysis\n\n\n\nAdroitly\n\nMission Analysis\n\n\n\nHas the AI hype train achieved escape velocity? An analysis of SpaceX‚Äôs orbital data centre for AGI training\n\n\n\n\n\nFeb 15, 2026\n\n\nGabriel\n\n\n\n\n\n\n\n\n\n\n\n\nSatellite Tasking MCP Server\n\n\n\nAdroitly\n\nMCP\n\noptimisation\n\n\n\nBuilding a Model Context Protocol server for satellite image tasking with natural language and agentic workflows\n\n\n\n\n\nJan 30, 2026\n\n\nGabriel\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Systems engineer turned entrepreneur with expertise in spacecraft and radar systems. I‚Äôve spent my career developing optimal solutions to complex problems where physics meets engineering constraints, personally driving technical innovation and leading teams to do the same.\nMy path has taken me from space mission designer to Program Director, leading teams through every phase of satellite development: writing proposals to secure funding, leading system design, hardware development and testing from benchtop into the wild, then coordinating 40+ people through launch and early operations. Along the way, I‚Äôve kept my technical edge sharpened‚Äîpersonally developing optimisation tools for satellite design and operations, and a novel signal processing architecture that I‚Äôve presented at international workshops.\nIn 2024, I founded my own consulting practice, Adroitly Consulting. We solve optimization problems across the space and radar sector‚Äîand increasingly beyond‚Äîdelivering at a pace augmented by AI. The thread that runs through our work: turning messy operational requirements into tractable problems with practical solutions.\nAvid rock climber‚Äîan activity where physics meets biomechanical constraints, and an opportunity to interact with nature."
  },
  {
    "objectID": "about.html#hi-im-gabriel",
    "href": "about.html#hi-im-gabriel",
    "title": "About Me",
    "section": "",
    "text": "Systems engineer turned entrepreneur with expertise in spacecraft and radar systems. I‚Äôve spent my career developing optimal solutions to complex problems where physics meets engineering constraints, personally driving technical innovation and leading teams to do the same.\nMy path has taken me from space mission designer to Program Director, leading teams through every phase of satellite development: writing proposals to secure funding, leading system design, hardware development and testing from benchtop into the wild, then coordinating 40+ people through launch and early operations. Along the way, I‚Äôve kept my technical edge sharpened‚Äîpersonally developing optimisation tools for satellite design and operations, and a novel signal processing architecture that I‚Äôve presented at international workshops.\nIn 2024, I founded my own consulting practice, Adroitly Consulting. We solve optimization problems across the space and radar sector‚Äîand increasingly beyond‚Äîdelivering at a pace augmented by AI. The thread that runs through our work: turning messy operational requirements into tractable problems with practical solutions.\nAvid rock climber‚Äîan activity where physics meets biomechanical constraints, and an opportunity to interact with nature."
  },
  {
    "objectID": "about.html#connect",
    "href": "about.html#connect",
    "title": "About Me",
    "section": "Connect",
    "text": "Connect\n\n  LinkedIn \n  GitHub"
  },
  {
    "objectID": "adroitly/2026-01-30-tasking-mcp/index.html",
    "href": "adroitly/2026-01-30-tasking-mcp/index.html",
    "title": "Satellite Tasking MCP Server",
    "section": "",
    "text": "credit: Nano Banana"
  },
  {
    "objectID": "adroitly/2026-01-30-tasking-mcp/index.html#high-revisit-imaging",
    "href": "adroitly/2026-01-30-tasking-mcp/index.html#high-revisit-imaging",
    "title": "Satellite Tasking MCP Server",
    "section": "High-revisit Imaging",
    "text": "High-revisit Imaging\nWhen considering sub-meter, high resolution imagery, optimizing revisit frequency directly increases revenue per satellite asset for several important applications.\nFirstly, some applications are inherently time sensitive: agriculture, deforestation monitoring, and disaster response require timely updates on the order of minutes for critical decisions to be made. An example would be the detection of man-made fires in the forests of Riau, Indonesia - an area spanning over 80,000 square kilometers - authorities need to know where to deploy their limited resources to combat this illegal farming practice before further harm occurs.\nSecondly, applications in predictive analytics and anomaly detection require multiple satellite images of a location to build up a baseline pattern-of-life. A short temporal sampling period on the order of hours would provide information on intra-day patterns - particularly useful for urban areas.\nFinally, advanced imagery products such as Synthetic Aperture Radar (SAR) tomography or interferometry require the acquisition of images in a short timeframe, to minimise temporal decorrelation of the scene."
  },
  {
    "objectID": "adroitly/2026-01-30-tasking-mcp/index.html#mixed-sensor-constellations",
    "href": "adroitly/2026-01-30-tasking-mcp/index.html#mixed-sensor-constellations",
    "title": "Satellite Tasking MCP Server",
    "section": "Mixed Sensor Constellations",
    "text": "Mixed Sensor Constellations\nMixed sensor constellations can be used to meet a single mission objective, often to better results than a homogeneous sensor constellation. For example, a mix of SAR and Electro-Optical (EO) sensors would be complementary in surveillance applications - EO providing easily-interpretable daytime imagery, and the active SAR sensor providing that all-weather, day-or-night imaging capability.\nThe benefits of mixed sensor constellations are not without its operational drawbacks. Operating multiple sensors - especially from different vendors - would imply dealing with different tasking workflows or different satellite constraints. This increases the cognitive load on the operator, particularly when the time pressure of high-revisit imaging is involved."
  },
  {
    "objectID": "adroitly/2026-01-30-tasking-mcp/index.html#putting-it-together-the-mixed-sensor-constellation-scheduling-problem",
    "href": "adroitly/2026-01-30-tasking-mcp/index.html#putting-it-together-the-mixed-sensor-constellation-scheduling-problem",
    "title": "Satellite Tasking MCP Server",
    "section": "Putting It Together: The Mixed Sensor Constellation Scheduling Problem",
    "text": "Putting It Together: The Mixed Sensor Constellation Scheduling Problem\nFrom our earlier discussions, a practical optimisation problem emerges: using a (potentially heterogeneous) constellation of satellite sensors, how can an operator find the optimal sequence of imaging tasks to yield high-revisit rates over a (long) list of Areas Of Interest (AOIs)?\nIn practice, AOIs evolve or are replaced with other locations over time. Sensors come online, or become unavailable due to maintenance downtime. This implies that an optimal task schedule is not produced once, but optimised for on a regular basis. In the extreme case, operators might be attempting to solve the scheduling problem at every orbital period - some assistance and automation is clearly required."
  },
  {
    "objectID": "adroitly/2026-01-30-tasking-mcp/index.html#aoi-creation",
    "href": "adroitly/2026-01-30-tasking-mcp/index.html#aoi-creation",
    "title": "Satellite Tasking MCP Server",
    "section": "AOI Creation",
    "text": "AOI Creation\nAs with typical tasking workflows, the operator is able to create AOIs on the map (shown as blue boxes). Through a tool in the MCP server, Claude can see the indicated AOIs.\n\nAs an alternate workflow step, the operator can specify the AOIs directly to Claude through the chat interface through latitude-longitude coordinates, or location name. Claude creates the AOIs using another MCP tool."
  },
  {
    "objectID": "adroitly/2026-01-30-tasking-mcp/index.html#constellation-discovery-and-imaging-mode-selection",
    "href": "adroitly/2026-01-30-tasking-mcp/index.html#constellation-discovery-and-imaging-mode-selection",
    "title": "Satellite Tasking MCP Server",
    "section": "Constellation Discovery and Imaging Mode Selection",
    "text": "Constellation Discovery and Imaging Mode Selection\nThe MCP server provides resources and prompts to Claude, enabling it to:\n\nDiscover the available constellation assets and their imaging modes\nRecommend suitable imaging modes and satellites in the constellation, based on the tasking context (i.e.¬†port monitoring in this scenario)."
  },
  {
    "objectID": "adroitly/2026-01-30-tasking-mcp/index.html#tasking-optimisation",
    "href": "adroitly/2026-01-30-tasking-mcp/index.html#tasking-optimisation",
    "title": "Satellite Tasking MCP Server",
    "section": "Tasking Optimisation",
    "text": "Tasking Optimisation\nWith AOI locations and constellation capabilities in its context, Claude can now optimise the task schedule. The resulting task schedule is presented to operator in the GUI for approval in a table (top left of figure), and color-coding (green AOIs are scheduled, red AOIs are not).\nThe optimisation backend considers the last-seen times for each AOI, providing a task schedule that aims to maximise the revisit frequency across all the AOIs. The constellation is utilised cooperatively, such that AOIs are not repeated between any two satellites.\nAOIs can also be given priority (not shown), which allows operators to meet varying levels of Quality of Service (QoS) in a single task schedule.\n\nAdditional visualisation functionality is provided, providing the operator an indication of imaging geometry."
  },
  {
    "objectID": "adroitly/2026-01-30-tasking-mcp/index.html#tool-chaining-and-agentic-workflows",
    "href": "adroitly/2026-01-30-tasking-mcp/index.html#tool-chaining-and-agentic-workflows",
    "title": "Satellite Tasking MCP Server",
    "section": "Tool Chaining and Agentic Workflows",
    "text": "Tool Chaining and Agentic Workflows\nIn the previous orbital period, one of the AOIs could not be fit into the task schedule. The operator can trigger an agentic workflow by simply asking for a tasking in the next available access window.\nClaude then chains several tools together to first find the next available access for this AOI, and then propose a task schedule for it.\nNotice in the chat interface that Claude is capable of proposing options: either the earliest access window available, or another access window with better geometry for downstream exploitation."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Can You Beat TOTO? A Statistical Analysis of Singapore‚Äôs National Lottery\n\n\n\nData Analysis\n\nStatistics\n\nProbability\n\nSingapore\n\n\n\nAnalyzing 17 years of TOTO draw data to determine if lottery numbers can be predicted and whether the house always wins\n\n\n\n\n\nJan 27, 2026\n\n\nGabriel\n\n\n\n\n\n\n\n\n\n\n\n\nThe First Post\n\n\n\n\n\nNew Year, New Me\n\n\n\n\n\nJan 27, 2026\n\n\nGabriel\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2026-01-27-TOTO-Lottery-Analysis/index.html",
    "href": "posts/2026-01-27-TOTO-Lottery-Analysis/index.html",
    "title": "Can You Beat TOTO? A Statistical Analysis of Singapore‚Äôs National Lottery",
    "section": "",
    "text": "credit: Nano Banana"
  },
  {
    "objectID": "posts/2026-01-27-TOTO-Lottery-Analysis/index.html#prize-structure",
    "href": "posts/2026-01-27-TOTO-Lottery-Analysis/index.html#prize-structure",
    "title": "Can You Beat TOTO? A Statistical Analysis of Singapore‚Äôs National Lottery",
    "section": "Prize Structure",
    "text": "Prize Structure\nTOTO distinguishes the first 6 numbers from the 7th additional number for the purposes of prize payouts. Here is the prize structure, direct from Singapore Pools (TOTO‚Äôs managing organisation).\n\n\n\nPrize structure table from Singapore Pools\n\n\nNote that Groups 1 to 4 receive a payout that is a percentage of the Prize Pool, while Groups 5 to 7 receive a fixed payout.\nAlso note that Groups 1 to 4 benefit from ‚Äúsnowballing‚Äù, in which un-won prize money from previous draws are rolled over to the current draw (limited to 4 rollovers).\nFinally, note that Group 1‚Äôs jackpot payout is guaranteed by Singapore Pools to payout a minimum of $1,000,000. The top-up of the prize amount will come from Singapore Pools‚Äô own pocket."
  },
  {
    "objectID": "posts/2026-01-27-TOTO-Lottery-Analysis/index.html#probability-of-winning",
    "href": "posts/2026-01-27-TOTO-Lottery-Analysis/index.html#probability-of-winning",
    "title": "Can You Beat TOTO? A Statistical Analysis of Singapore‚Äôs National Lottery",
    "section": "Probability of Winning",
    "text": "Probability of Winning\nAs a risk averse (i.e.¬†kiasu) Singaporean, I want to play the game knowing my chances. So lets math it out. Here we will make heavy use of the binomial coefficient formula to determine how many combinations are possible when picking \\(r\\) numbers from a set of \\(n\\) numbers. The formula is:\n\\[\nC(n,r)=\\frac{n\\times(n-1)\\times  \\dots\\times (n-r+1)}{r \\times (r-1) \\times \\dots  \\times 1}\n\\]\nThe number of combinations for a single TOTO ticket of six numbers is \\(C(49,6)=13,983,816\\).\nWith reference to the prize structure table above, lets calculate the number of combinations to win Group 7. First you would need to match three of the six winning numbers, so \\(C(6,3)=20\\). Then not match the additional number, so \\(C(1,0)=1\\). You still have to fill your ticket with three more numbers, which can be any combination of the 42 remaining undrawn numbers, so \\(C(42,3)=11,480\\). Hence, the number of ways you can win Group 7 is:\n\\[\n\\text{Number of Ways to Win Group 7} = C(6,3)\\times C(1,0) \\times C(42,3)=229,600\n\\]\nThe probability of winning Group 7 is the number of ways to win it divided by the total possible ways to form a TOTO ticket, so \\(229600/13983816 = 1.64\\%\\)\nLets do the same calculation for all the prize groups.\n\n\n\nGroup\nNumber of Ways to Win the Group\nProbability \\(P(Win_i)\\)\n\n\n\n\n1\n\\(C(6,6)\\times C(1,0) \\times C(42,0)=1\\)\n0.000007 %\n\n\n2\n\\(C(6,5)\\times C(1,1) \\times C(42,0)=6\\)\n0.00004 %\n\n\n3\n\\(C(6,5)\\times C(1,0) \\times C(42,1)=252\\)\n0.0018 %\n\n\n4\n\\(C(6,4)\\times C(1,1) \\times C(42,1)=630\\)\n0.0045 %\n\n\n5\n\\(C(6,4)\\times C(1,0) \\times C(42,2)=12915\\)\n0.0924 %\n\n\n6\n\\(C(6,3)\\times C(1,1) \\times C(42,2)=17220\\)\n0.1231 %\n\n\n7\n\\(C(6,3)\\times C(1,0) \\times C(42,3)=229600\\)\n1.6419 %\n\n\n\nSumming the probabilities, the chance of winning any prize is approximately 1.84%. That‚Äôs only marginally better than flipping a coin and getting tails six times in a row."
  },
  {
    "objectID": "posts/2026-01-27-TOTO-Lottery-Analysis/index.html#expected-value-of-winning",
    "href": "posts/2026-01-27-TOTO-Lottery-Analysis/index.html#expected-value-of-winning",
    "title": "Can You Beat TOTO? A Statistical Analysis of Singapore‚Äôs National Lottery",
    "section": "Expected Value of Winning",
    "text": "Expected Value of Winning\nOkay, but even if the odds are small, we should keep playing because the payouts are big, right?\nWe can calculate the expected value for Groups 5 to 7 easily, since they have fixed payouts of $50 to $10. For Group 7,\n\\(\\mathbb{E}[Payout_7]=P(Win_7) \\times Prize_7 = 0.016419 \\times \\$10 = \\$0.16\\)\nWhat about Groups 1 to 4, with a variable prize which is a percentage of the total prize pool? First, we must recognise that:\n\nThe prize pool is 54.5% of the total sales $\\(N\\) (more on this later)\nOne ticket is $1\n\nLet the percentage of the prize pool for group \\(i\\) be \\(Pct_i\\), then\n\\(\\mathbb{E}[Payout_i] = P(Win_i)\\times \\mathbb{E}[Prize_i]\\)\n\\(\\mathbb{E}[Payout_i] = P(Win_i)\\times \\frac{N\\times 0.545\\times Pct_i}{N \\times P(Win_i)}\\)\n\\(\\mathbb{E}[Payout_i] = 0.545 \\times Pct_i\\)\n\n\n\nGroup\nProbability\n\\(\\mathbb{E}[Payout_i]\\)\n\n\n\n\n1\n0.000007 %\n$0.2071\n\n\n2\n0.00004 %\n$0.0436\n\n\n3\n0.0018 %\n$0.0300\n\n\n4\n0.0045 %\n$0.0164\n\n\n5\n0.0924 %\n$0.0462\n\n\n6\n0.1231 %\n$0.0308\n\n\n7\n1.6419 %\n$0.1642\n\n\n\nAnd so the expected value of winning any group is $0.54. Statistically speaking, if you totalled your profits and losses across your natural lifetime, you would only get back 54% of what you put in. Ouch. But what if - like my dear grandmother - you had a system for picking those TOTO numbers? Could you ‚Äúbeat the odds‚Äù?"
  },
  {
    "objectID": "posts/2026-01-27-TOTO-Lottery-Analysis/index.html#hot-and-cold-numbers",
    "href": "posts/2026-01-27-TOTO-Lottery-Analysis/index.html#hot-and-cold-numbers",
    "title": "Can You Beat TOTO? A Statistical Analysis of Singapore‚Äôs National Lottery",
    "section": "Hot and Cold Numbers",
    "text": "Hot and Cold Numbers\nOne method of picking TOTO numbers is based off the frequency they are drawn. Some people think numbers that occur more frequently will continue to occur frequently (hot), while others think numbers that occur less frequently are due to appear soon (cold).\nBut what is ‚Äúmore‚Äù or ‚Äúless‚Äù frequent?\nTo determine if a number occurs more or less frequently, we have to compare it to some expected frequency. Lets work out what this expected frequency is.\nFor a number \\(i\\), the probability of \\(i\\) being drawn is \\(P(i \\text{ is drawn}) = 7/49\\), because there are 7 chances to draw a number, and there are 49 TOTO numbers. There are two possible outcomes for the number \\(i\\), drawn or not drawn. This means that each number follows a binomial distribution.\nThrough careful inspection of the 49 balls in the cool glass contraption in the image above, we can hypothesize that each number is equally likely to be drawn. Furthermore, picking a number \\(i\\) does not affect the probability of picking a number \\(j\\), so all 49 TOTO numbers are independently and identically distributed (i.i.d).\nFor a binomial distribution, the average or expected frequency is \\(np\\), where \\(n\\) is the number of TOTO lottery draws (1154 in our dataset), and \\(p = P(i \\text{ is drawn}) = 7/49\\). Then the expected frequency is:\n\\[\n\\mathbb{E}[i \\text{ is drawn in dataset}] = np = 1154\\times 7/49=164.86\n\\]\nLets see what the data shows.\n\n\n\nNumber frequency distribution showing hot and cold numbers\n\n\nHere we see a plot for all 49 TOTO numbers on the horizontal axis, and the number of occurrences on the vertical axis. The red horizontal line depicts the expected number of occurrences for the dataset (164.86). Hot numbers appear above the line, Cold numbers appear below the line.\nThe top 6 hottest and coldest numbers are labelled for you lucky cats out there.\nIs the data statistically significant?\nBefore you rush off to dump your life savings into the next TOTO draw, lets check whether the variation we are seeing could be explainable by the variance in the underlying probability distribution. The variance of a binomial distribution is calculated as:\n\\[\nVar[i\\text{ is drawn in dataset}] = np(1-p) = 1154 \\times 7/49 \\times 42/49 = 141.3\n\\]\nAnd the standard deviation is thus \\(\\sigma_n=\\sqrt{141.3}=11.88\\). For the ‚Äúhottest‚Äù TOTO number 15 appearing 189 times in the dataset means that it is \\((189-164.86)/11.88 = 2.03\\sigma\\), or approximately 2 standard deviations from the mean.\nIn other words, if a TOTO number truly does follow a binomial distribution (and the number of TOTO draws is large like in our dataset here), then we would expect occurrences of \\(2\\sigma\\) to appear approximately 5% of the time, or 2 to 3 of the 49 TOTO numbers. Indeed, the data shows TOTO numbers 15 and 45 are the two numbers that deviate by \\(\\geq2\\sigma\\).\nDoubling Down‚Ä¶ on Math\n‚ÄúBut wait!‚Äù you say, putting your TOTO tickets aside for a moment. ‚ÄúThat‚Äôs just back-of-the-envelope math, and all I have is this TOTO ticket to scribble on! Maybe there are other hot or cold numbers?‚Äù\nYou‚Äôre right, we‚Äôve only looked at the hottest and coldest numbers so far. A complementary check we could perform would be the Chi-Square Test, denoted by the formula\n\\[\n\\chi^2 = \\sum_{i=1}^{49}\\frac{(o_i - e_i)^2}{e_i}\n\\]\nWhere \\(o_i\\) and \\(e_i\\) are the observed and expected number of occurrences for the TOTO number \\(i\\) respectively.\nWhile previously we were looking at individual outlying TOTO numbers, here the Chi-Square test checks all 49 TOTO numbers to see if each and every TOTO number follows a binomial distribution. More specifically, the value of \\(\\chi^2\\) represents the sum of deviations from the expected occurrences, which can be checked against the Chi-Square distribution to obtain the probability that all TOTO numbers have a fair chance of being drawn (which we call the null hypothesis).\nFrom calculation, \\(\\chi^2=32.73\\), and with \\(49-1=48\\) degrees of freedom, the p-value obtained is \\(95.48\\%\\) (probability of the dataset representing samples of a fair TOTO draw).\nUnfortunately, the data shows that TOTO is very likely not rigged, and every TOTO number has an equally likely probability of occurring.\nA Slight Detour into Psychology\nGambling is interesting to me because its analysis sits at the intersection of math and psychology. This perception of hot and cold numbers has been comprehensively documented under the term ‚ÄúThe Gambler‚Äôs Fallacy‚Äù. The little voice in our head insisting that previously absent numbers should appear soon suggests some link between previous TOTO draws and current ones. That the mathematical reality is that each draw is independent from past and future draws also requires some manner of belief; in the infallible logic of math."
  },
  {
    "objectID": "posts/2026-01-27-TOTO-Lottery-Analysis/index.html#but-wait-i-have-a-system",
    "href": "posts/2026-01-27-TOTO-Lottery-Analysis/index.html#but-wait-i-have-a-system",
    "title": "Can You Beat TOTO? A Statistical Analysis of Singapore‚Äôs National Lottery",
    "section": "But wait, I have a SYSTEM!",
    "text": "But wait, I have a SYSTEM!\nWe‚Äôve already shown that the TOTO game is mathematically fair, with strong evidence from the 15 years of historical results. That does not mean that TOTO is impossible to win, merely that any combination of six TOTO numbers are equally likely to win. And for some people, spending a dollar to ‚Äúbuy hope‚Äù (‰π∞Â∏åÊúõ in mandarin) is still worth it. Lets conclude this section with some data visualisation. Instead of letting the TOTO system pick numbers for you at random, maybe you can use the figure below to pick for yourself. But really, I just want to show you some colourful plots I made.\nThe system I propose is based on co-occurrence, and is detailed as follows:\n\nStarting from a hand-picked initial number \\(a\\), you find the most commonly paired number \\(b\\).\nFrom \\(b\\), find the next most commonly paired number \\(c\\).\nRepeat until you get all six numbers for your TOTO ticket\nProfit???\n\nLets first visualise the co-occurrences of every pair of numbers in a heatmap plot. The plot is symmetrical about its diagonal, because the number of occurrences for TOTO number pair \\((a,b)\\) is the same as pair \\((b,a)\\). The diagonal for pair \\((a,a)\\) is ignored and arbitrarily set to 0. Green boxes indicate pairs that are drawn frequently, while red boxes indicate rarely drawn pairs.\n\n\n\nHeatmap of co-occurrence between TOTO number pairs\n\n\nWe should also note that all pairs have been seen at least once. The actual co-occurrence ranges from 9 to 40.\nThe heatmap contains useful information, but is difficult to read in detail. We can visualise it as a network graph, with each node representing a single TOTO number. Connections (i.e.¬†edges) between the TOTO numbers represent pairs, and the closer the nodes are to each other, the more frequently the corresponding pairs occur in the dataset. The larger the node, the more connections it has.\nNote that we are only displaying the top 20% of edges, because the graph is fully connected and the visualisation would be really challenging to read otherwise.\n\n\n\nNetwork graph showing TOTO number co-occurrence relationships\n\n\nNow we are ready to pick numbers. Starting from your favourite number, traverse the network until you get all six TOTO numbers on your ticket. If you selected each node based on node size, this means you now have the most historically frequent set of numbers which also include your favourite number!\nReminder: past performance does not guarantee future performance, and this system is by no means mathematically optimal. Although if you do win, please consider buying me a coffee (with a sprinkling of gold flakes and diamonds in it)."
  },
  {
    "objectID": "posts/2026-01-27-TOTO-Lottery-Analysis/index.html#loss-conditions-for-singapore-pools",
    "href": "posts/2026-01-27-TOTO-Lottery-Analysis/index.html#loss-conditions-for-singapore-pools",
    "title": "Can You Beat TOTO? A Statistical Analysis of Singapore‚Äôs National Lottery",
    "section": "Loss Conditions for Singapore Pools",
    "text": "Loss Conditions for Singapore Pools\nLets first define losses to Singapore Pools, then we can find the conditions in which they occur.\nDefinition 1: For a given TOTO draw, the total payouts exceed the allotted 54.5% of total sales. This is the ‚Äúweak‚Äù version of loss, since it just means Singapore Pools obtains less income for that draw.\nDefinition 2: For a given TOTO draw, the total payouts exceed total sales. This is the ‚Äústrong‚Äù version of loss, since Singapore Pools experienced negative income and would have to dig into their treasury to service the payouts owed to the ticket winners.\n\nConditions for ‚ÄúWeak‚Äù Loss\nInspecting the TOTO rules, there are two scenarios in which Singapore Pools would suffer a weak loss. Some combination of these scenarios would be required for a weak loss per draw.\n\nTicket sales are so low for a draw that Singapore Pools have to top up the Group 1 prize in order to meet the minimum amount of \\(1M. This occurs when\\) &lt; 10^6$ \\((N\\times 0.545) \\times 0.38 &lt; 10^6\\) \\(N &lt; 4,828,585 \\text{ tickets}\\)\nThe total (fixed amount) payouts for groups 5 to 7 exceed the allotted amount of 24.80% of total sales, requiring payouts taken from the 45.5% of total sales skimmed off for Singapore Pool‚Äôs income (this scenario assumes groups 1-4 are paid out as well). \\(0.248 N &lt; Payout_{5,6,7} &lt; 0.703N\\)\n\nBefore we can analyse the data, we need to create the required features first. The dataset only provides Group \\(i\\) payouts and number of winners - thankfully, we have sufficient information to find the ticket sales (i.e.¬†the value of \\(N\\)). Within the time range of the dataset, there has always been at least one winner in group 3, meaning the prize payouts have never been rolled over (‚Äúsnowballed‚Äù). Knowing that group 3 is paidout 3% of total sales, we can find \\(N=Prize_3 / 0.03\\).\n\n\nHistorical Occurrences of ‚ÄúWeak‚Äù Loss\nCondition 1: Low Ticket Sales\nNow, lets see if condition 1 for weak loss has ever occurred in our dataset. In the following figure, we plot the total sales amount, compared against the $4.8M sales threshold where topup of group 1 prize is required.\n\n\n\nTicket sales over time with $4.8M threshold indicated\n\n\nIn total, 12 draws were found that required topup. 10 of those draws occurred in the period of July to September 2020, which was the year when Singapore locked down for Covid-19. Looks like Singaporeans were sensible enough to prioritise their health over queueing for a TOTO ticket in those tumultuous times.\n2 July 2020 marked the lowest total sales in the history of the updated game at only $4.05M. Singapore Pools had to topup $161K into the group 1 prize amount. (Unfortunately, no one won the group 1 prize on that draw)\nCondition 2: Many Group 5 to 7 Winners\nSince the prize pool is 54.5% of total sales, and groups 1 to 4 are allocated 54.5% of the prize pool, it follows that groups 5 to 7 are allocated the remaining 45.5% of the prize pool.\n\n\n\nPrize pool allocation between prize groups over time\n\n\nFrom the dataset, 43% of the TOTO draws see Singapore Pools suffering a weak loss. In draws where Singapore Pools loses, we also see larger variance in the payouts, such as 2 May 2016 where group 5 to 7 payouts exceeded 90% of the total prize pool.\nInterestingly, the average payouts for groups 5 to 7 across the dataset is 46.1% of the price pool, which is 0.6% above the allocated payout amount.\nBoth Conditions Combined\nThe actual impact of a weak loss can be measured when we assess both conditions as a whole. This is achieved by plotting the payouts for all groups as a percentage of total sales. With reference to the TOTO rules, Singapore Pools expects the sum of all payouts to be ‚â§54.5% of total sales.\n\n\n\nTotal payouts as percentage of sales showing Singapore Pools win/loss ratio\n\n\nHere we see that Singapore Pools win 61% of the draws (win meaning it takes more than the expected 54.5% as income), an increase from the 56% house-win rate from condition 2. This is likely due to the draws where groups 1 to 4 do not have any winners - the savings from not having to payout those larger prizes offsets the small-prize payouts of groups 5 to 7.\nNote also that there have been no payouts &gt;100% of total sales, which means that Singapore Pools have never experienced the ‚Äústrong‚Äù definition of loss in any draw. By this definition of loss, Singapore Pools (a.k.a. ‚Äúthe House‚Äù) has indeed always won.\nYear-on-Year Income\nThus far we have been analysing the dataset per-draw, whereas corporate accounting typically analyses the income on an annual basis.\n\n\n\nYear-on-year actual vs expected income for Singapore Pools\n\n\nIn this figure, we see Singapore Pools‚Äô actual income versus expected income (45% of total sales) for each full year, from 2015 to 2025. The difference between actual and expected income is also plotted in the red lineplot. 7 out of the 10 years assessed showed lower-than-expected income, indicating weak loss.\n\n\nConditions for ‚ÄúStrong‚Äù Loss\nFrom analysis of the data, we now know that total payouts have never exceeded total sales per draw - Singapore Pools have never experienced a strong loss. But could it though? Are there conditions that increase the probability of a strong loss occurring?\nRecall that the condition for a strong loss is expressed as:\n\\[\n\\sum_{i=1}^7 \\text{payout}_i &gt; N\n\\]\nWhere \\(N\\) is ticket sales ($1 per ticket), and \\(i\\) represents the prize group number.\nSince groups 1 to 4 prizes are defined as a function of the prize pool, payouts will never exceed the allocation (ignoring the $1M topup rule for simplicity).\nIt thus falls to groups 5 to 7 with fixed prizes to win more than their allocation, expressed as:\n\\[\n\\sum_{i=5}^7 \\text{payout}_i &gt; N -\\sum_{i=1}^4 \\text{payout}_i\n\\]\n\\[\n\\sum_{i=5}^7 \\text{payout}_i &gt; N -(N\\times 0.545 \\times 0.545)\n\\]\n\\[\n\\sum_{i=5}^7 \\text{payout}_i &gt; 0.703N\n\\]\nLets re-write \\(\\text{payout}_i\\) to be more explicit.\n\\[\n50W_5 + 25W_6 + 10W_7 &gt; 0.703N\n\\]\nWhere \\(W_i\\) represents the number of winning tickets per group \\(i\\), multiplied by the fixed prize payouts for each group.\nWe should also note that \\(W_i=X_1 + X_2 + \\dots + X_N\\), where each \\(X_k \\in [0,1]\\) is an indicator variable on whether ticket \\(k\\) has won in group \\(i\\). This implies that \\(X_k\\) is a Bernoulli random variable, and \\(W_i\\) thus follows a Binomial distribution.\nBut if \\(N\\) is large enough, then Central Limit Theorem allows us to treat \\(W_i\\) as normally distributed! The rule of thumb here is \\(Np\\geq 10\\) and \\(N(1-p)\\geq 10\\). Lets check this.\n\n\n\n\n\n\n\n\n\n\nGroup \\(i\\)\nLowest recorded \\(N\\)\n\\(p_i=P(Win_i)\\)\n\\(Np_i\\)\n\\(N(1-p_i)\\)\n\n\n\n\n5\n4048473\n0.00092357\n3739\n4044733\n\n\n6\n4048473\n0.00123142\n4985\n4043487\n\n\n7\n4048473\n0.01641898\n66471\n3982001\n\n\n\nSince \\(Np &gt;&gt;10\\) and \\(N(1-p) &gt;&gt; 10\\), it‚Äôs safe to proceed with Central Limit Theorem!\n\\(W_i\\) is approximately normal, with mean \\(\\mu_i=\\mathbb{E}[Win_i]\\) and variance \\(\\sigma_i^2=Np_i(1-p_i)\\).\nAnd by extension, fixed prize payouts \\(50W_5 + 25W_6 + 10W_7\\) are also normally distributed, with mean \\(\\mu=\\sum_{i=5}^7\\mathbb{E}[Win_i]\\) and variance \\(\\sigma^2=\\sum_{i=5}^7Np_i(1-p_i)\\).\nLets expand the terms.\n\\[\n\\mu = 50Np_5 + 25Np_6 + 10Np_7 = 0.241N\n\\]\n\\[\n\\sigma^2 = 50^2Np_5(1-p_5)+25^2Np_6(1-p_6) + 10^2Np_7(1-p_7) = 4.69N\n\\]\n\\[\n\\sigma=\\sqrt{\\sigma^2}=2.17\\sqrt{N}\n\\]\nHaving approximated the fixed price payouts as normally distributed, we can find the Z-score via conversion to a standard normal distribution. The strong loss threshold we are looking at is \\(0.703N\\), which converts to the Z-threshold as \\(\\frac{0.703N-\\mu}{\\sigma} = 0.213\\sqrt{N}\\)\nTo be clear, we have made the equivalence that \\(P(payout &gt; 0.703N) = P(Z &gt; 0.213\\sqrt{N})\\)\nWhat‚Äôs the probability of strong loss occurring?\nWell, it depends. In the following figure we plot the probability \\(P(Z &gt; 0.213\\sqrt{N})\\) for varying values of \\(N\\).\n\n\n\nProbability of strong loss occurring as a function of ticket sales\n\n\nNote that the probability of a strong loss occurring increases as ticket sales decreases. This probability drops below 5% after 60 tickets have been sold, and below 1% after 120 tickets. That‚Äôs a ‚Äúsure win‚Äù for Singapore Pools to me.\nBut wait! How is it that when ticket sales are smaller, there‚Äôs a higher probability of Singapore Pools suffering a strong loss? That‚Äôs confusing.\n\n\n\ncredit: Crystal_ YouTube\n\n\nBut here‚Äôs the thing: Singapore Pools suffers a strong loss as a proportion of \\(N\\).\n\\[\n\\text{Strong Loss Occurs} \\rightarrow \\text{Payout Groups 5 to 7} &gt; 0.703N\n\\]\nEquivalently,\n\\[\n\\text{Payout Groups 5 to 7}/N &gt; 0.703\n\\]\nThe average payout for fixed prize groups just has to be larger than 70.3cents.\nNote also that the standard deviation reduces as \\(N\\) increases.\n\nStandard deviation of fixed prize payouts by ticket sales\n\n\n\\(N\\)\n\\(\\sigma = 2.17\\sqrt{N}\\)\n\n\n\n\n100\n$0.217\n\n\n10,000\n$0.0217\n\n\n1,000,000\n$0.00068\n\n\n\nSo the ‚Äúspread‚Äù about the mean actually reduces when \\(N\\) is large, meaning it gets harder to achieve that 70.3 cents per ticket payout. That‚Äôs some 200IQ play, Singapore Pools."
  },
  {
    "objectID": "posts/2026-01-27-TOTO-Lottery-Analysis/index.html#so-when-does-the-house-lose",
    "href": "posts/2026-01-27-TOTO-Lottery-Analysis/index.html#so-when-does-the-house-lose",
    "title": "Can You Beat TOTO? A Statistical Analysis of Singapore‚Äôs National Lottery",
    "section": "So, When Does the House Lose?",
    "text": "So, When Does the House Lose?\nYou can make this happen! Don‚Äôt pray play to the TOTO god. When ticket sales are low enough, there is (A) a certainty that Singapore Pools will have to top up the group 1 prize, and (B) a greater chance that fixed prize payouts exceed the total prize pool.\nWill this ever happen? I doubt so. Even if ticket sales are abnormally low, Singapore Pools can top up the prize pool (to beyond $1M), thus incentivising more players into the draw and stacking the odds in their favour."
  }
]